{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b280926e-121f-4bf5-8c79-ae6468a437c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd86daa-40aa-43ab-9da3-2986e24af11a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following imports are not used in this notebook:\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, GBTClassificationModel, RandomForestClassificationModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2b3d99-1ab5-4b33-b896-f08fc9ef0958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# databricks requires us to set SPARKML_TEMP_DFS_PATH, but this cannot be done via spark config on serverless compute so we use this workaround, same with MLFLOW_DFS_TMP\n",
    "import os\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/pophealthrisk/pophealthrisk/pophealthrisk/ml-temp\"\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/pophealthrisk/pophealthrisk/pophealthrisk/ml-temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27e00b45-5335-4c70-9ef3-189998a87b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Hyperparameter tuning approach\n",
    "As we have a large amount of data we will take a train, validate, test approach, as we can assume that sampling effects will be minimal. For smaller data sets, where sampling bias could have an impact, a cross-validation approach would be more appropriate. \n",
    "\n",
    "# Model choice\n",
    "We consider gradient boosted trees and random forest classifiers as these often perform well on this kind of tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d808df5-4e9a-4779-a7bc-d69a40f6c37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# balance dataset\n",
    "balanced_df = df_min.sampleBy(\"RFHLTH_adj\", fractions={0: 0.25, 1: 1}, seed=10)\n",
    "balanced_df.groupBy('RFHLTH_adj').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7137cd90-34e0-4587-ac0c-ddccd065489d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "train, test =balanced_df.randomSplit(weights=[0.8,0.2], seed=200)\n",
    "print(test.count())\n",
    "print(train.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeed6f7a-dc76-49cf-a365-4b69a530a7e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols=[\"_AGEG5YR_clean\",\"EDUCA_clean\",'_BMI5', '_SMOKER3_clean', 'DRNKANY6_clean','INCOME3_clean','num_conditions','income_adj_pov']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_assembled = assembler.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8915e15f-fc36-4d80-8a18-aec7c8e69911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"RFHLTH_adj\",\n",
    "    rawPredictionCol=\"probability\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62e5fa15-3594-4bd5-913b-e3af1c473352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"RFHLTH_adj\",\n",
    "    seed=42\n",
    ")\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [100, 200, 300])\n",
    "             .addGrid(rf.maxDepth, [4,6,8])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6182768-195c-4828-8263-c5c4b0fd70d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(\n",
    "    labelCol=\"RFHLTH_adj\",\n",
    "    seed=42\n",
    ")\n",
    "paramGridGbt = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxIter, [20,40,60,80])\n",
    "             .addGrid(gbt.maxDepth, [4,6,8])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "befcb05c-96b0-4f71-9b55-b0993f815e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_gbt = CrossValidator(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=paramGridGbt,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # or 5\n",
    "    parallelism=4\n",
    ")\n",
    "cv_gbt_fit = cv_gbt.fit(train_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71c6c8d-a94a-4490-a6ee-82a0d5284b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cv_rf = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,  # or 5\n",
    "    parallelism=4\n",
    ")\n",
    "cv_rf_fit = cv_rf.fit(train_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b95aa1d-a888-419b-98e3-5d1d19636d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_results = [\n",
    "    (param[rf.numTrees], param[rf.maxDepth], metric)\n",
    "    for param, metric in zip(paramGrid, cv_rf_fit.avgMetrics)\n",
    "]\n",
    "rf_df = pd.DataFrame(rf_results, columns=[\"Trees_or_Iters\", \"maxDepth\", \"Mean_F1\"])\n",
    "rf_df[\"Model\"] = \"RandomForest\"\n",
    "\n",
    "# --- GBT results ---\n",
    "gbt_results = [\n",
    "    (param[gbt.maxIter], param[gbt.maxDepth], metric)\n",
    "    for param, metric in zip(paramGridGbt, cv_gbt_fit.avgMetrics)\n",
    "]\n",
    "gbt_df = pd.DataFrame(gbt_results, columns=[\"Trees_or_Iters\", \"maxDepth\", \"Mean_F1\"])\n",
    "gbt_df[\"Model\"] = \"GBT\"\n",
    "\n",
    "# Combine results\n",
    "results_df = pd.concat([rf_df, gbt_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed0b96d-894b-40bd-adf3-7d6f62feb4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.lineplot(\n",
    "    data=results_df,\n",
    "    x=\"Trees_or_Iters\",\n",
    "    y=\"Mean_F1\",\n",
    "    hue=\"Model\",\n",
    "    style=\"maxDepth\",\n",
    "    markers=True,\n",
    "    dashes=False\n",
    ")\n",
    "plt.title(\"Cross-validated F1 Comparison: Random Forest vs GBT\")\n",
    "plt.ylabel(\"Mean F1 Score\")\n",
    "plt.xlabel(\"numTrees (RF) / maxIter (GBT)\")\n",
    "plt.legend(title=\"Model / maxDepth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b7fcf2f-e7f2-4a59-bdd9-9da85c7169c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# train final model\n",
    "gbt_optimal =  GBTClassifier(\n",
    "    labelCol=\"RFHLTH_adj\",\n",
    "    seed=42,\n",
    "    maxIter=80,\n",
    "    maxDepth=4\n",
    ")\n",
    "final_model = gbt_optimal.fit(train_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec5d442-4e74-4824-8b88-6f1ed85de9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save model, this is a quick save so that we do not need to retrain every time\n",
    "final_model.write().overwrite().save(\"/Volumes/pophealthrisk/pophealthrisk/pophealthrisk/models/gbt-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e76b256a-1c66-49a9-8a38-c6b75c48cc85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "final_model = GBTClassificationModel.load(\"/Volumes/pophealthrisk/pophealthrisk/pophealthrisk/models/gbt-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b64a27-4dd6-4215-8a0b-30fecbcff204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_assembled = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2054a5-28c2-48b5-8b28-7198c8591969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = final_model.transform(test_assembled)\n",
    "evaluator.setMetricName(\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(preds)\n",
    "print(\"Validation AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7559156a-b218-4258-9d7d-860ddd5853cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = preds.filter(\n",
    "    F.col(\"prediction\") == F.col(\"RFHLTH_adj\")\n",
    ").count() / preds.count()\n",
    "\n",
    "# True Positives, False Positives, False Negatives\n",
    "tp = preds.filter((F.col(\"prediction\") == 1) & (F.col(\"RFHLTH_adj\") == 1)).count()\n",
    "fp = preds.filter((F.col(\"prediction\") == 1) & (F.col(\"RFHLTH_adj\") == 0)).count()\n",
    "fn = preds.filter((F.col(\"prediction\") == 0) & (F.col(\"RFHLTH_adj\") == 1)).count()\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-score: {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_df = preds.groupBy(\n",
    "    \"RFHLTH_adj\", \"prediction\"\n",
    ").count().groupBy(\n",
    "    \"RFHLTH_adj\"\n",
    ").pivot(\n",
    "    \"prediction\"\n",
    ").sum(\"count\").fillna(0)\n",
    "\n",
    "display(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5302375-582c-42dd-8818-3f86d155649d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get probability and label columns as pandas DataFrame\n",
    "roc_data = (\n",
    "    preds\n",
    "    .select(\"RFHLTH_adj\", \"probability\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Extract probability for class 1\n",
    "roc_data[\"prob_1\"] = roc_data[\"probability\"].apply(lambda x: float(x[1]))\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(roc_data[\"RFHLTH_adj\"], roc_data[\"prob_1\"])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275f8a25-bba6-4abb-8bae-3f8779975024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log with mlflow for deployment\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "example_df = test_assembled.limit(1).toPandas()\n",
    "predicted_df = final_model.transform(test_assembled.limit(1)).toPandas()\n",
    "signature = infer_signature(example_df, predicted_df)\n",
    "\n",
    "model_info = mlflow.spark.log_model(final_model, \"PHR_GBT\",signature=signature)\n",
    "mlflow.register_model(model_info.model_uri, \"PopHealthRisk.PopHealthRisk.PHR_GBT\")\n",
    "\n",
    "client = MlflowClient()\n",
    "client.set_registered_model_alias(\"pophealthrisk.pophealthrisk.phr_gbt\", \"prod\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f4c479-ded3-403d-b3a3-cbb6715d2c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Summary\n",
    "Predictive performance is limited but good enough for this exercise. Due to balancing of the training set, we have fairly balanced predictions without bias. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/rowan.howell@gmail.com/PopHealthRisk/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
